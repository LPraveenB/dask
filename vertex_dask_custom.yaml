# PIPELINE DEFINITION
# Name: dask-dataframe
# Inputs:
#    container_uri: str
#    job_name: str
#    job_suffix: str
#    master_machine_type: str
#    num_workers: int
#    project_id: str
#    project_location: str
#    replica_count: int
#    staging_bucket: str
#    worker_machine_type: str
components:
  comp-dask-dataframe:
    executorLabel: exec-dask-dataframe
    inputDefinitions:
      parameters:
        container_uri:
          parameterType: STRING
        job_name:
          parameterType: STRING
        job_suffix:
          parameterType: STRING
        master_machine_type:
          parameterType: STRING
        num_workers:
          parameterType: NUMBER_INTEGER
        project_id:
          parameterType: STRING
        project_location:
          parameterType: STRING
        replica_count:
          parameterType: NUMBER_INTEGER
        staging_bucket:
          parameterType: STRING
        worker_machine_type:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-dask-dataframe:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - dask_dataframe
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform==1.24.1'\
          \ 'kfp==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)

          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef dask_dataframe(\n        project_id: str,\n        project_location:\
          \ str,\n        container_uri: str,\n        staging_bucket: str,\n    \
          \    job_name: str,\n        job_suffix: str,\n        replica_count: int,\n\
          \        master_machine_type: str,\n        worker_machine_type: str,\n\
          \        num_workers: int,\n):\n    \"\"\"Run a custom training job using\
          \ a training script.\n    \"\"\"\n    import json\n    import logging\n\
          \    import os.path\n    import time\n    import google.cloud.aiplatform\
          \ as aip\n\n    worker_pool_specs = [\n        {\n            \"machine_spec\"\
          : {\n                \"machine_type\": master_machine_type,\n          \
          \  },\n            \"replica_count\": 1,\n            \"container_spec\"\
          : {\n                \"image_uri\": container_uri,\n                \"command\"\
          : ['bash', 'entrypoint.sh'],\n                \"args\": [\n            \
          \        '--run_name', job_name + '_' + job_suffix,\n                  \
          \  '--num_workers', str(num_workers),\n                    '--steps', 'evaluate'\n\
          \                ],\n            },\n        },\n        {\n           \
          \ \"machine_spec\": {\n                \"machine_type\": worker_machine_type,\n\
          \            },\n            \"replica_count\": replica_count,\n       \
          \     \"container_spec\": {\n                \"image_uri\": container_uri,\n\
          \                \"command\": ['bash', 'entrypoint.sh'],\n             \
          \   \"args\": [\n                    '--num_workers', str(num_workers),\n\
          \                ],\n            },\n        }\n    ]\n\n    # my_job =\
          \ aip.CustomJob(\n    #     display_name=job_name + '_' + job_suffix,\n\
          \    #     worker_pool_specs=worker_pool_specs,\n    #     staging_bucket=staging_bucket,\n\
          \    #     project=project_id,\n    #     location=project_location\n  \
          \  # )\n\n    # my_job.run()\n\n"
        image: python:3.7
pipelineInfo:
  name: dask-dataframe
root:
  dag:
    tasks:
      dask-dataframe:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-dask-dataframe
        inputs:
          parameters:
            container_uri:
              componentInputParameter: container_uri
            job_name:
              componentInputParameter: job_name
            job_suffix:
              componentInputParameter: job_suffix
            master_machine_type:
              componentInputParameter: master_machine_type
            num_workers:
              componentInputParameter: num_workers
            project_id:
              componentInputParameter: project_id
            project_location:
              componentInputParameter: project_location
            replica_count:
              componentInputParameter: replica_count
            staging_bucket:
              componentInputParameter: staging_bucket
            worker_machine_type:
              componentInputParameter: worker_machine_type
        taskInfo:
          name: dask-dataframe
  inputDefinitions:
    parameters:
      container_uri:
        parameterType: STRING
      job_name:
        parameterType: STRING
      job_suffix:
        parameterType: STRING
      master_machine_type:
        parameterType: STRING
      num_workers:
        parameterType: NUMBER_INTEGER
      project_id:
        parameterType: STRING
      project_location:
        parameterType: STRING
      replica_count:
        parameterType: NUMBER_INTEGER
      staging_bucket:
        parameterType: STRING
      worker_machine_type:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.0.0
